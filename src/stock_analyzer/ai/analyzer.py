"""
===================================
Aè‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ - AIåˆ†æå±‚
===================================

èŒè´£ï¼š
1. ä½¿ç”¨ litellm è°ƒç”¨å¤šç§ LLM API è¿›è¡Œè‚¡ç¥¨åˆ†æ
2. æ”¯æŒ 100+ providersï¼ˆdeepseek, gemini, openai, anthropic ç­‰ï¼‰
3. æ”¯æŒä¸»å¤‡æ¨¡å‹è‡ªåŠ¨å›é€€
4. ç»“åˆæŠ€æœ¯é¢å’Œæ¶ˆæ¯é¢ç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

import logging
import time
from typing import Any

from stock_analyzer.ai.clients import LiteLLMClient
from stock_analyzer.ai.prompt_builder import PromptBuilder
from stock_analyzer.ai.response_parser import ResponseParser
from stock_analyzer.ai.snapshot_builder import MarketSnapshotBuilder
from stock_analyzer.config import get_config
from stock_analyzer.domain import get_stock_name_from_context
from stock_analyzer.domain.entities.analysis_result import AnalysisResult
from stock_analyzer.domain.services.interfaces import IAIAnalyzer
from stock_analyzer.utils.fallback import create_sequential_fallback

logger = logging.getLogger(__name__)


class AIAnalyzer(IAIAnalyzer):
    """
    AI åˆ†æå™¨ - åŸºäº litellm çš„å¤š provider æ”¯æŒ

    èŒè´£ï¼š
    1. è°ƒç”¨é…ç½®çš„ LLM API è¿›è¡Œè‚¡ç¥¨åˆ†æ
    2. æ”¯æŒä¸»å¤‡æ¨¡å‹è‡ªåŠ¨å›é€€
    3. ç»“åˆé¢„å…ˆæœç´¢çš„æ–°é—»å’ŒæŠ€æœ¯é¢æ•°æ®ç”Ÿæˆåˆ†ææŠ¥å‘Š
    4. è§£æ AI è¿”å›çš„ JSON æ ¼å¼ç»“æœ

    ä½¿ç”¨æ–¹å¼ï¼š
        analyzer = AIAnalyzer()
        result = analyzer.analyze(context, news_context)
    """

    def __init__(self):
        """
        åˆå§‹åŒ– AI åˆ†æå™¨

        è‡ªåŠ¨ä»é…ç½®è¯»å–ä¸»æ¨¡å‹å’Œå¤‡é€‰æ¨¡å‹é…ç½®
        """
        config = get_config()

        # åˆå§‹åŒ–ä¸»æ¨¡å‹å®¢æˆ·ç«¯
        self._primary_client = LiteLLMClient(
            model=config.ai.llm_model,
            api_key=config.ai.llm_api_key,
            base_url=config.ai.llm_base_url,
        )

        # åˆå§‹åŒ–å¤‡é€‰æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        self._fallback_client: LiteLLMClient | None = None
        if config.ai.llm_fallback_model and config.ai.llm_fallback_api_key:
            self._fallback_client = LiteLLMClient(
                model=config.ai.llm_fallback_model,
                api_key=config.ai.llm_fallback_api_key,
                base_url=config.ai.llm_fallback_base_url,
            )

        # é¢„åˆ›å»ºå›é€€ç­–ç•¥å¯¹è±¡
        self._fallback_handler = create_sequential_fallback(name="ai_llm_fallback")

        # æ£€æŸ¥å¯ç”¨æ€§
        if not self.is_available():
            logger.warning("æœªé…ç½®æœ‰æ•ˆçš„ LLM API Keyï¼ŒAI åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")
        else:
            logger.info(f"AI åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ (ä¸»æ¨¡å‹: {config.ai.llm_model})")
            if self._fallback_client and self._fallback_client.is_available():
                logger.info(f"å¤‡é€‰æ¨¡å‹å·²é…ç½®: {config.ai.llm_fallback_model}")

    def is_available(self) -> bool:
        """æ£€æŸ¥åˆ†æå™¨æ˜¯å¦å¯ç”¨"""
        return self._primary_client.is_available() or (
            self._fallback_client is not None and self._fallback_client.is_available()
        )

    def _call_api(self, prompt: str, generation_config: dict) -> str:
        """
        è°ƒç”¨ AI APIï¼Œæ”¯æŒä¸»å¤‡æ¨¡å‹å›é€€

        Args:
            prompt: æç¤ºè¯
            generation_config: ç”Ÿæˆé…ç½®

        Returns:
            å“åº”æ–‡æœ¬
        """
        if not self.is_available():
            raise Exception("æ²¡æœ‰å¯ç”¨çš„ AI å®¢æˆ·ç«¯")

        def call_primary() -> str:
            return self._primary_client.generate(prompt, generation_config)

        # å¦‚æœæœ‰å¤‡é€‰æ¨¡å‹ä¸”ä¸»æ¨¡å‹å¯ç”¨ï¼Œä½¿ç”¨å›é€€ç­–ç•¥
        if self._fallback_client and self._fallback_client.is_available():

            def call_fallback() -> str:
                if self._fallback_client is None:
                    raise Exception("å¤‡é€‰æ¨¡å‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                return self._fallback_client.generate(prompt, generation_config)

            if self._primary_client.is_available():
                return self._fallback_handler.execute(call_primary, call_fallback)
            else:
                return call_fallback()

        # åªæœ‰ä¸»æ¨¡å‹
        if self._primary_client.is_available():
            return call_primary()

        raise Exception("æ²¡æœ‰å¯ç”¨çš„ AI å®¢æˆ·ç«¯")

    def analyze(self, context: dict[str, Any], news_context: str | None = None) -> AnalysisResult:
        """
        åˆ†æå•åªè‚¡ç¥¨

        æµç¨‹ï¼š
        1. æ ¼å¼åŒ–è¾“å…¥æ•°æ®ï¼ˆæŠ€æœ¯é¢ + æ–°é—»ï¼‰
        2. è°ƒç”¨ LLM APIï¼ˆå¸¦é‡è¯•å’Œæ¨¡å‹å›é€€ï¼‰
        3. è§£æ JSON å“åº”
        4. è¿”å›ç»“æ„åŒ–ç»“æœ

        Args:
            context: ä» storage.get_analysis_context() è·å–çš„ä¸Šä¸‹æ–‡æ•°æ®
            news_context: é¢„å…ˆæœç´¢çš„æ–°é—»å†…å®¹ï¼ˆå¯é€‰ï¼‰

        Returns:
            AnalysisResult å¯¹è±¡
        """
        code = context.get("code", "Unknown")
        config = get_config()

        # è¯·æ±‚å‰å¢åŠ å»¶æ—¶
        request_delay = config.ai.llm_request_delay
        if request_delay > 0:
            logger.debug(f"[LLM] è¯·æ±‚å‰ç­‰å¾… {request_delay:.1f} ç§’...")
            time.sleep(request_delay)

        # è·å–è‚¡ç¥¨åç§°
        name = context.get("stock_name")
        if not name or name.startswith("è‚¡ç¥¨"):
            if "realtime" in context and context["realtime"].get("name"):
                name = context["realtime"]["name"]
            else:
                name = get_stock_name_from_context(code, context)

        # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤ç»“æœ
        if not self.is_available():
            return AnalysisResult(
                code=code,
                name=name,
                sentiment_score=50,
                trend_prediction="éœ‡è¡",
                operation_advice="æŒæœ‰",
                confidence_level="ä½",
                analysis_summary="AI åˆ†æåŠŸèƒ½æœªå¯ç”¨ï¼ˆæœªé…ç½® API Keyï¼‰",
                risk_warning="è¯·é…ç½® LLM_API_KEY åé‡è¯•",
                success=False,
                error_message="LLM API Key æœªé…ç½®",
            )

        try:
            # æ ¼å¼åŒ–è¾“å…¥
            prompt = PromptBuilder.build_analysis_prompt(context, name, news_context)

            # è·å–æ¨¡å‹åç§°
            model_name = config.ai.llm_model

            logger.info(f"========== AI åˆ†æ {name}({code}) ==========")
            logger.info(f"[LLMé…ç½®] æ¨¡å‹: {model_name}")
            logger.info(f"[LLMé…ç½®] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"[LLMé…ç½®] æ˜¯å¦åŒ…å«æ–°é—»: {'æ˜¯' if news_context else 'å¦'}")

            # è®°å½•å®Œæ•´ prompt
            prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
            logger.info(f"[LLM Prompt é¢„è§ˆ]\n{prompt_preview}")
            logger.debug(f"=== å®Œæ•´ Prompt ({len(prompt)}å­—ç¬¦) ===\n{prompt}\n=== End Prompt ===")

            # è®¾ç½®ç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": config.ai.llm_temperature,
                "max_output_tokens": config.ai.llm_max_tokens,
            }

            logger.info(f"[LLMè°ƒç”¨] å¼€å§‹è°ƒç”¨ {model_name} API...")

            # ä½¿ç”¨å¸¦é‡è¯•çš„ API è°ƒç”¨
            start_time = time.time()
            response_text = self._call_api(prompt, generation_config)
            elapsed = time.time() - start_time

            logger.info(f"[LLMè¿”å›] {model_name} API å“åº”æˆåŠŸ, è€—æ—¶ {elapsed:.2f}s, å“åº”é•¿åº¦ {len(response_text)} å­—ç¬¦")

            # è®°å½•å“åº”é¢„è§ˆ
            response_preview = response_text[:300] + "..." if len(response_text) > 300 else response_text
            logger.info(f"[LLMè¿”å› é¢„è§ˆ]\n{response_preview}")
            logger.debug(
                f"=== {model_name} å®Œæ•´å“åº” ({len(response_text)}å­—ç¬¦) ===\n{response_text}\n=== End Response ==="
            )

            # è§£æå“åº”
            result = ResponseParser.parse(response_text, code, name)
            result.raw_response = response_text
            result.search_performed = bool(news_context)
            result.market_snapshot = MarketSnapshotBuilder.build(context)

            logger.info(f"[LLMè§£æ] {name}({code}) åˆ†æå®Œæˆ: {result.trend_prediction}, è¯„åˆ† {result.sentiment_score}")

            return result

        except Exception as e:
            logger.error(f"AI åˆ†æ {name}({code}) å¤±è´¥: {e}")
            return AnalysisResult(
                code=code,
                name=name,
                sentiment_score=50,
                trend_prediction="éœ‡è¡",
                operation_advice="æŒæœ‰",
                confidence_level="ä½",
                analysis_summary=f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)[:100]}",
                risk_warning="åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•æˆ–æ‰‹åŠ¨åˆ†æ",
                success=False,
                error_message=str(e),
            )

    def batch_analyze(
        self,
        contexts: list[dict[str, Any]],
        delay_between: float = 2.0,
        news_contexts: list[str | None] | None = None,
    ) -> list[AnalysisResult]:
        """
        æ‰¹é‡åˆ†æå¤šåªè‚¡ç¥¨

        Args:
            contexts: ä¸Šä¸‹æ–‡æ•°æ®åˆ—è¡¨
            delay_between: æ¯æ¬¡åˆ†æä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
            news_contexts: æ–°é—»ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆä¸contextsä¸€ä¸€å¯¹åº”ï¼‰

        Returns:
            AnalysisResult åˆ—è¡¨
        """
        results = []

        for i, context in enumerate(contexts):
            if i > 0:
                logger.debug(f"ç­‰å¾… {delay_between} ç§’åç»§ç»­...")
                time.sleep(delay_between)

            news_context = news_contexts[i] if news_contexts and i < len(news_contexts) else None
            result = self.analyze(context, news_context=news_context)
            results.append(result)

        return results

    def generate_market_review(self, prompt: str, generation_config: dict[str, Any]) -> str | None:
        """
        ç”Ÿæˆå¸‚åœºå¤ç›˜æŠ¥å‘Š

        Args:
            prompt: æç¤ºè¯
            generation_config: ç”Ÿæˆé…ç½®

        Returns:
            ç”Ÿæˆçš„å¤ç›˜æŠ¥å‘Šæ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            return self._call_api(prompt, generation_config)
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¸‚åœºå¤ç›˜æŠ¥å‘Šå¤±è´¥: {e}")
            return None


# ä¾¿æ·å‡½æ•°
def get_analyzer() -> AIAnalyzer:
    """è·å– AI åˆ†æå™¨å®ä¾‹"""
    return AIAnalyzer()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.DEBUG)

    test_context = {
        "code": "600519",
        "date": "2026-01-09",
        "today": {
            "open": 1800.0,
            "high": 1850.0,
            "low": 1780.0,
            "close": 1820.0,
            "volume": 10000000,
            "amount": 18200000000,
            "pct_chg": 1.5,
            "ma5": 1810.0,
            "ma10": 1800.0,
            "ma20": 1790.0,
            "volume_ratio": 1.2,
        },
        "ma_status": "å¤šå¤´æ’åˆ— ğŸ“ˆ",
        "volume_change_ratio": 1.3,
        "price_change_ratio": 1.5,
    }

    analyzer = AIAnalyzer()

    if analyzer.is_available():
        print("=== AI åˆ†ææµ‹è¯• ===")
        result = analyzer.analyze(test_context)
        print(f"åˆ†æç»“æœ: {result.to_dict()}")
    else:
        print("LLM API æœªé…ç½®ï¼Œè·³è¿‡æµ‹è¯•")
